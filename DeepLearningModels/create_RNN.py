# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wZd4ISNCIsebrsyOpuHbTj-NV3Q4yadI
"""
import io
import json
import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam


#Import dataset
df = pd.read_csv('OpenAi/amlo_clasify_chatpgt3.csv')

#Select the classification we want
class_text = ['opinion']

#reduce data selection
df = df[df['classification_spanish'].isin(class_text)]
df = df.sample(2000)

#join every text in a variable
data = ' '.join(df['Texto']).splitlines(keepends= True )


#create a tokenizer for the data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data)

# words to their token
encoded_text = tokenizer.texts_to_sequences(data) 


vocab_size = len(tokenizer.word_counts) + 1

# from the text we generate groups of words similar to n-grams
datalist = []

for d in encoded_text:
  if len(d) > 1:
    for i in range(2, len(d)):
      datalist.append(d[:i])

#we fill the vectors with zeros at the beginning
max_len = 20
sequences = pad_sequences(datalist, maxlen=max_len, padding= 'pre')

#Transform data to fit data in the model
X = sequences[:, :-1]
y = sequences[:, -1]
y = to_categorical(y, num_classes=vocab_size)
seq_len = X.shape[1]

# create the model
model = Sequential()
model.add(Embedding(vocab_size, 50, input_length = seq_len))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(vocab_size, activation='softmax'))

#compile the model
model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])
# train model
model.fit(X,y, batch_size=32, epochs=120)


#Generate text
text_len = 20

def generate_text(seed_text, n_lines):

  for i in range(n_lines):
    text = []
    for _ in range(text_len):
      encoded = tokenizer.texts_to_sequences([seed_text])
      encoded = pad_sequences(encoded, maxlen=seq_len, padding="pre")
      y_pred = np.argmax(model.predict(encoded, verbose =0), axis=-1)
      predicted_word = ""
      for word, index in tokenizer.word_index.items():
        if index == y_pred:
          predicted_word = word
          break
      seed_text = seed_text + " " + predicted_word
      text.append(predicted_word)
    seed_text = text[-1]
    text = " ".join(text)
    print(text)

#save model
model.save('RNN_model.h5')

#save tokenizer
tokenizer_json = tokenizer.to_json()
with io.open('tokenizer_rnn.json', 'w', encoding='iso-8859-1') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

#save sequence_len
with open('seq_len.txt', 'w') as f:
    f.write(str(seq_len))


